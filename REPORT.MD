# Лабораторная работа 6: Файловые системы FUSE

## 1. Цель работы

Изучить архитектуру виртуальной файловой системы (VFS) Linux и научиться создавать пользовательские файловые системы с использованием FUSE (Filesystem in Userspace). Понять принципы работы файловых операций, взаимодействие между ядром и пространством пользователя, а также оценить производительность userspace файловых систем.

---

## 2. Теоретическая часть

### 2.1. Архитектура VFS и роль FUSE

**VFS (Virtual File System)** — это абстрактный слой в ядре Linux, который скрывает детали конкретных файловых систем (ext4, tmpfs, btrfs и т.д.) и предоставляет единый интерфейс для системных вызовов (`open`, `read`, `write`, `stat`, `unlink` и т.п.).

Основные задачи VFS:

- обеспечить единый API для всех файловых систем;
- маршрутизировать операции к нужной конкретной FS;
- кэшировать метаданные (dentry cache, inode cache);
- управлять объектами вроде `inode`, `dentry`, `file`, `super_block`.

**FUSE (Filesystem in Userspace)** — это механизм, который позволяет реализовать файловую систему в пространстве пользователя. В ядре есть модуль `fuse`, который выступает как “прокси” между VFS и пользовательским демоном (наш процесс `myfuse`).

Особенности FUSE:

- ядро пересылает запросы VFS (getattr, read, write, readdir и т.д.) в userspace;
- пользовательский процесс реализует операции и возвращает результат обратно в ядро;
- нет необходимости писать kernel-модуль, достаточно обычного пользовательского приложения.

### 2.2. Схема взаимодействия

Логическая схема обработки операций в FUSE:

```text
+-------------------+
|  Пользовательское |
|     приложение    |
| (cat, echo, dd)   |
+---------+---------+
          |
          | системный вызов (open, read, write, ...)
          v
+---------+---------+
|        VFS        |
+---------+---------+
          |
          | перенаправление в FUSE FS
          v
+---------+---------+
|   FUSE kernel     |
|     module        |
+---------+---------+
          |
          | обмен через /dev/fuse
          v
+-------------------+
|  FUSE daemon      |
| (наш myfuse в     |
|  пространстве     |
|  пользователя)    |
+---------+---------+
          |
          | операции с реальной FS
          v
+-------------------+
|  Реальная FS      |
| (/tmp/source,     |
|  tmpfs в данном   |
|  стенде)          |
+-------------------+
````

### 2.3. Реализованные file operations

В работе реализованы следующие операции FUSE:

* `getattr` — получение атрибутов файла/директории (обёртка над `lstat` реальной FS).
* `readdir` — чтение содержимого директории (`opendir`, `readdir` + заполнение `filler`).
* `open` — открытие файла, хранение файлового дескриптора в `fi->fh`.
* `read` — чтение данных (`pread` по `fd`), с учётом режима:

  * passthrough — данные не изменяются;
  * `rot13` — применяется `rot13_transform`;
  * `upper` — применяется `uppercase_transform`.
* `write` — запись данных (`pwrite`), при режиме ROT13 данные шифруются перед записью.
* `create` — создание файла (реализация через `open(..., O_CREAT, mode)`).
* `unlink` — удаление файла (`unlink` на реальной FS).
* `mkdir` — создание директории (`mkdir`).
* `rmdir` — удаление директории (`rmdir`).
* `release` — закрытие файла (`close(fd)`), освобождение ресурса.

Все операции используют вспомогательную функцию `build_full_path`, которая безопасно строит путь внутри базовой директории (`/tmp/source`), запрещая `..` (защита от path traversal).

---

## 3. Ход выполнения

### 3.1. Подготовка окружения

Работа выполнялась в среде:

* `uname -a`:

  ```text
  Linux armiqu-VirtualBox 6.14.0-35-generic #35-Ubuntu SMP PREEMPT_DYNAMIC Sat Oct 11 10:06:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
  ```
* Файловая система для `/tmp`:

  ```text
  df -T /tmp
  tmpfs  /tmp
  ```

Установлены зависимости:

```bash
sudo apt update
sudo apt install -y pkg-config libfuse3-dev
```

Созданы директории:

```bash
sudo mkdir -p /mnt/fuse
sudo chown "$USER":"$USER" /mnt/fuse
mkdir -p /tmp/source
```

Проект собран командой:

```bash
cd lab6
make
```

### 3.2. Задание A: Passthrough FUSE

#### Реализация

В режиме **passthrough** (переменная `MYFUSE_MODE` не задана) файловая система просто проксирует все операции в реальную директорию `/tmp/source`. Все пути строятся через `build_full_path`, привязывающую FUSE-путь (`/file.txt`) к базе (`/tmp/source/file.txt`).

При чтении и записи данные не изменяются. Логирование операций реализовано через функцию `log_operation`, которая пишет в `stderr` строки вида:

```text
[YYYY-MM-DD HH:MM:SS] OPERATION: /path (success|errno)
```

(Из-за демонизации FUSE реальные строки в файле лога не сохранились, но формат и вызовы функции присутствуют в коде.)

#### Команды для запуска

```bash
cd ~/lab6
./myfuse /tmp/source /mnt/fuse
```

#### Тестирование

Во втором терминале:

```bash
echo "test" > /mnt/fuse/file.txt
cat /mnt/fuse/file.txt
ls -l /mnt/fuse
ls -l /tmp/source
rm /mnt/fuse/file.txt
ls -l /mnt/fuse
ls -l /tmp/source
```

Фактический вывод:

```text
cat /mnt/fuse/file.txt
test
ls -l /mnt/fuse
итого 4
-rw-rw-r-- 1 armiqu armiqu 5 дек  4 12:15 file.txt

ls -l /tmp/source
итого 4
-rw-rw-r-- 1 armiqu armiqu 5 дек  4 12:15 file.txt

rm /mnt/fuse/file.txt

ls -l /mnt/fuse
итого 0

ls -l /tmp/source
итого 0
```

Наблюдение: файл создаётся, читается и удаляется корректно и через FUSE, и напрямую.

---

### 3.3. Задание B: ROT13 Encryption Filesystem

#### Реализация

При режиме `MYFUSE_MODE=rot13` чтение/запись проходят через ROT13-преобразование:

* В `write` данные копируются во временный буфер, к ним применяется `rot13_transform`, и уже зашифрованный текст записывается в реальную FS.
* В `read` данные читаются с диска и затем подвергаются `rot13_transform` перед возвратом приложению.

На диске (`/tmp/source`) данные всегда в виде ROT13, а через `/mnt/fuse` — в обычном виде.

#### Команды для запуска

```bash
cd ~/lab6
MYFUSE_MODE=rot13 ./myfuse /tmp/source /mnt/fuse
```

#### Тестирование

```bash
echo "Hello World" > /mnt/fuse/secret.txt

echo "Реальный файл (на /tmp/source):"
cat /tmp/source/secret.txt

echo "Через FUSE:"
cat /mnt/fuse/secret.txt
```

Фактический результат:

```text
Реальный файл (на /tmp/source):
Uryyb Jbeyq
Через FUSE:
Hello World
```

То есть требование задания B выполнено: данные зашифрованы в реальной FS и автоматически расшифровываются при чтении через FUSE.

---

### 3.4. Задание C: Uppercase Filesystem

#### Реализация

При режиме `MYFUSE_MODE=upper` изменяется только операция `read`:

* В `myfs_read` после чтения данных из реальной FS к буферу применяется `uppercase_transform`, переводящая латинские буквы `a–z` в `A–Z`.
* Операции записи и хранения данных в реальной FS не изменяются.

Таким образом, на диске файлы лежат “как есть”, а через FUSE содержимое всегда отображается в верхнем регистре.

#### Команды для запуска

```bash
cd ~/lab6
MYFUSE_MODE=upper ./myfuse /tmp/source /mnt/fuse
```

#### Тестирование

```bash
echo "hello world" > /tmp/source/test_upper.txt

echo "Обычное чтение:"
cat /tmp/source/test_upper.txt

echo "Через FUSE:"
cat /mnt/fuse/test_upper.txt
```

Результат:

```text
Обычное чтение:
hello world
Через FUSE:
HELLO WORLD
```

Требование задания C (uppercase только при чтении) выполнено.

---

## 4. Нагрузочное тестирование

### 4.1. Тестовый стенд

* ОС: Ubuntu в VirtualBox
* Ядро: `Linux 6.14.0-35-generic`
* Реальная FS: `/tmp` на `tmpfs`
* Базовая директория FUSE: `/tmp/source`
* Точка монтирования: `/mnt/fuse`
* Режим: passthrough (`MYFUSE_MODE` не задана)

### 4.2. Методика измерений

Для измерения использовались команды `dd` и `/usr/bin/time -p`.

1. **Latency (Тест 1)**
   Чтение файла размером ~205 MB блоками по 4 KB, `count=50000`:

   * через FUSE: `/mnt/fuse/testfile`
   * напрямую: `/tmp/source/testfile`

2. **Throughput (Тест 2)**
   Последовательная запись/чтение файлов размеров:

   * 1 MB, 10 MB, 100 MB
     через:
   * `/mnt/fuse/...` (FUSE)
   * `/tmp/source/...` (реальная FS)

3. **IOPS (Тест 3)**
   Создание 1000 маленьких файлов по ~несколько байт:

   * в `/mnt/fuse/iops_fuse`
   * в `/tmp/source/iops_ext4`

### 4.3. Результаты измерений

#### 4.3.1. Latency операций (read 4KB)

Команды и результаты:

```bash
# через FUSE
/usr/bin/time -p dd if=/mnt/fuse/testfile of=/dev/null bs=4096 count=50000
50000+0 records in
50000+0 records out
204800000 bytes (205 MB, 195 MiB) copied, 0,942797 s, 217 MB/s
real 0.94
user 0.03
sys 0.67

# напрямую
/usr/bin/time -p dd if=/tmp/source/testfile of=/dev/null bs=4096 count=50000
50000+0 records in
50000+0 records out
204800000 bytes (205 MB, 195 MiB) copied, 0,0692092 s, 3,0 GB/s
real 0.07
user 0.00
sys 0.06
```

Средняя задержка одной операции `read(4KB)`:

* FUSE: ( 0.94 \text{ s} / 50000 \approx 0.0188 \text{ ms} )
* Реальная FS: ( 0.07 \text{ s} / 50000 \approx 0.0014 \text{ ms} )

| Операция       | FS    | Время (real), с | Latency, мс |
| -------------- | ----- | --------------- | ----------: |
| read 4KB × 50k | FUSE  | 0.94            |      0.0188 |
| read 4KB × 50k | tmpfs | 0.07            |      0.0014 |

Можно увидеть порядка ~10–15× рост задержки на одну операцию при использовании FUSE.

---

#### 4.3.2. Throughput (скорость чтения/записи)

Результаты (как выводил `dd`):

**Размер 1 MB**

```bash
# WRITE FUSE 1MB
1048576 bytes ... copied, 0,00677333 s, 155 MB/s

# WRITE tmpfs 1MB
1048576 bytes ... copied, 0,00132332 s, 792 MB/s

# READ FUSE 1MB
1048576 bytes ... copied, 0,00230712 s, 454 MB/s

# READ tmpfs 1MB
1048576 bytes ... copied, 0,000990408 s, 1,1 GB/s
```

**Размер 10 MB**

```bash
# WRITE FUSE 10MB
10485760 bytes ... copied, 0,02608 s, 402 MB/s

# WRITE tmpfs 10MB
10485760 bytes ... copied, 0,0064114 s, 1,6 GB/s

# READ FUSE 10MB
10485760 bytes ... copied, 0,0261446 s, 401 MB/s

# READ tmpfs 10MB
10485760 bytes ... copied, 0,0016776 s, 6,3 GB/s
```

**Размер 100 MB**

```bash
# WRITE FUSE 100MB
104857600 bytes ... copied, 1,51625 s, 69,2 MB/s

# WRITE tmpfs 100MB
104857600 bytes ... copied, 0,68534 s, 153 MB/s

# READ FUSE 100MB
104857600 bytes ... copied, 0,220161 s, 476 MB/s

# READ tmpfs 100MB
104857600 bytes ... copied, 0,0280051 s, 3,7 GB/s
```

Сводная таблица (по данным `dd`):

| Размер | Операция | FS    | Скорость (примерно) |
| ------ | -------- | ----- | ------------------- |
| 1 MB   | write    | FUSE  | ~155 MB/s           |
| 1 MB   | write    | tmpfs | ~792 MB/s           |
| 1 MB   | read     | FUSE  | ~454 MB/s           |
| 1 MB   | read     | tmpfs | ~1.1 GB/s           |
| 10 MB  | write    | FUSE  | ~402 MB/s           |
| 10 MB  | write    | tmpfs | ~1.6 GB/s           |
| 10 MB  | read     | FUSE  | ~401 MB/s           |
| 10 MB  | read     | tmpfs | ~6.3 GB/s           |
| 100 MB | write    | FUSE  | ~69 MB/s            |
| 100 MB | write    | tmpfs | ~153 MB/s           |
| 100 MB | read     | FUSE  | ~476 MB/s           |
| 100 MB | read     | tmpfs | ~3.7 GB/s           |

Выводы:

* На больших файлах (100 MB) FUSE по чтению остаётся достаточно быстрым (сотни MB/s), но заметно уступает tmpfs.
* Overhead FUSE особенно заметен по записи больших объёмов (100 MB): скорость примерно в 2–2.5 раза ниже, чем у прямого доступа к tmpfs.

---

#### 4.3.3. IOPS (1000 мелких файлов)

Команды:

```bash
# через FUSE
/usr/bin/time -p bash -c 'for i in $(seq 1 1000); do echo "data" > /mnt/fuse/iops_fuse/file_$i; done'
real 0.87
user 0.04
sys 0.26

# напрямую
/usr/bin/time -p bash -c 'for i in $(seq 1 1000); do echo "data" > /tmp/source/iops_ext4/file_$i; done'
real 0.02
user 0.00
sys 0.02
```

Оценка IOPS:

* FUSE: ~1000 / 0.87 ≈ 1150 операций/с
* tmpfs: ~1000 / 0.02 ≈ 50 000 операций/с

| FS    | Кол-во файлов | Время (real), с | Оценка IOPS |
| ----- | ------------- | --------------- | ----------: |
| FUSE  | 1000          | 0.87            |       ~1150 |
| tmpfs | 1000          | 0.02            |     ~50 000 |

Здесь сильно видно, что FUSE хуже подходит для очень большого числа мелких операций.

---

### 4.4. Общие выводы по тестированию

1. **Overhead FUSE**:
   Время одной операции `read(4KB)` через FUSE примерно в 10–15 раз выше, чем при прямом доступе к tmpfs. Это связано с дополнительными копированиями и переключениями контекста между ядром и userspace.

2. **Большие файлы**:
   Для последовательного чтения больших файлов (десятки и сотни MB) FUSE показывает приемлемую скорость (сотни MB/s), хотя и заметно проигрывает нативной FS.

3. **Маленькие файлы и IOPS**:
   При большом количестве мелких файлов (1000 файлов) FUSE показывает низкий IOPS по сравнению с прямым доступом, что подтверждает, что bottleneck возникает именно на множестве мелких операций.

4. **tmpfs vs FUSE**:
   В данной конфигурации реальная FS — `tmpfs`, которая сама по себе очень быстрая. На её фоне overhead FUSE особенно заметен.

---

## 5. Ответы на контрольные вопросы

### Архитектура файловых систем

**1. Что такое VFS (Virtual File System) и зачем она нужна?**
VFS — это абстрактный слой в ядре Linux, который предоставляет единый интерфейс для работы с файлами и директориями, независимо от того, какая конкретная файловая система используется (ext4, tmpfs, btrfs и т.д.). Она позволяет приложениям работать с файлами одинаково, а ядру — подменять реализацию “под капотом”.

**2. Объясните разницу между inode, dentry и file descriptor.**

* `inode` — объект ядра, описывающий файл: его тип, права, владельца, размеры, таймстемпы, указатели на блоки данных.
* `dentry` (directory entry) — элемент каталога, связывающий имя (`"file.txt"`) с `inode`. dentry cache ускоряет поиск по именам.
* `file descriptor` (fd) — число в пространстве процесса, которое указывает на структуру `struct file` в ядре. Он представляет открытый файл/сокет и хранит текущий offset и флаги открытия.

**3. Что хранится в структуре `struct inode`?**
Типичные поля:

* тип файла (обычный, каталог, симлинк);
* права доступа (mode);
* uid/gid владельца;
* размер файла;
* таймстемпы (atime, mtime, ctime);
* ссылки (link count);
* указатели на блоки данных или другое описание размещения;
* указатель на `super_block`.

**4. Что такое superblock и какую информацию он содержит?**
`super_block` описывает файловую систему в целом: её тип, размер, размер блоков, положение структур на диске, ограничения, а также указатели на метод-таблицу операций FS (super_operations, inode_operations, file_operations).

**5. Как работает кеш dentry и зачем он нужен?**
Dentry cache хранит в памяти соответствие “путь/имя → inode”. При повторных обращениях к тем же именам файлов ядру не нужно заново проходить по каталогам на диске. Это сильно ускоряет операции вроде `stat`, `open`, `readdir`, уменьшает количество обращений к диску и системных вызовов.

---

### FUSE

**6. Как FUSE взаимодействует с ядром Linux?**
В ядре есть модуль `fuse`. Когда FUSE-файловая система монтируется, ядро создаёт специальное устройство (`/dev/fuse`) и связывает его с пользовательским процессом (демоном). Все VFS-операции (getattr, read, write, readdir, …) упаковываются ядром и отправляются через `/dev/fuse` в userspace. Процесс-демон обрабатывает запрос и отправляет ответ обратно через тот же канал.

**7. Опишите путь системного вызова `read()` в FUSE FS.**

1. Приложение вызывает `read(fd, buf, size)`.
2. Ядро через VFS находит соответствующую `struct file` и связанную FUSE FS.
3. VFS формирует запрос `FUSE_READ` и отправляет его в модуль `fuse`.
4. Модуль `fuse` передаёт запрос через `/dev/fuse` в пользовательский демон.
5. Демон вызывает свою реализацию `read` (в нашем случае `myfs_read`), которая читает данные из реальной FS (`pread`).
6. В зависимости от режима (passthrough/ROT13/UPPERCASE) данные могут быть преобразованы.
7. Результат возвращается через `/dev/fuse` обратно в ядро.
8. Ядро копирует данные в буфер приложения, и `read()` возвращает число прочитанных байт.

**8. Почему FUSE работает медленнее нативных kernel FS?**

* Дополнительные переключения контекста между ядром и userspace.
* Дополнительные копирования данных.
* Overhead протокола FUSE (упаковка/распаковка запросов).
* Пользовательский код обычно не такой оптимизированный, как kernel-код.

**9. Какие преимущества даёт разработка FS в userspace?**

* Не нужно писать и загружать kernel-модуль.
* Проще отлаживать (gdb, printf, логирование).
* Меньше риск “уронить” систему при ошибке.
* Портируемость (часть логики легко переносится).
* Быстрое прототипирование нестандартных FS (шифрование, overlay, view-FS и т.п.).

**10. Приведите примеры популярных FUSE файловых систем.**

* `sshfs` — монтирование удалённого каталога через SSH.
* `encfs`, `gocryptfs` — шифрованные FS в userspace.
* Облачные FUSE FS: `gcsfuse` (Google Cloud Storage), файловые системы для S3 и т.д.

---

### File operations

**11. Что делает операция `getattr()` и когда она вызывается?**
`getattr` возвращает атрибуты файла/директории (заполняет `struct stat`: размер, права, тип, таймстемпы и т.д.). Она вызывается при операциях вроде `stat()`, `ls -l`, `access()`, а также при многих других, когда ядру нужно узнать метаданные файла.

**12. В чём разница между `open()` и `create()`?**

* `open` — просто открывает существующий файл с заданными флагами.
* `create` — создаёт новый файл (обычно как комбинация `open` с `O_CREAT` и проверкой наличия). В FUSE есть отдельный callback `create`, который создаёт и сразу “открывает” файл.

**13. Почему `read()` принимает параметр `offset`?**
`offset` указывает, с какого смещения внутри файла нужно читать. Это позволяет:

* поддерживать случайный доступ (seek);
* делать несколько независимых чтений одного и того же файла;
* реализовывать корректную работу с несколькими процессами/потоками.

В FUSE `offset` приходит из ядра, а реализация обычно использует `pread`.

**14. Как `readdir()` возвращает список файлов?**
FUSE передаёт в `readdir` callback `fuse_fill_dir_t filler`. Реализация вызывает `filler(buf, name, ...)` для `"."`, `".."` и для каждого имени из `readdir(3)` реальной FS. Таким образом, ядро заполняет структуру каталога именами файлов.

**15. Что должна возвращать `getattr()` для директории?**
Для директории `st_mode` должен содержать бит `S_IFDIR`, а также права доступа (например, `0755`). Размер (`st_size`) может быть любым разумным, но главное — правильно указать тип (каталог) и права. Также должны быть корректно заполнены uid/gid, таймстемпы и количество ссылок.

---

### Метаданные и атрибуты

**16. Что такое атрибуты файла? Приведите примеры.**
Атрибуты — это метаданные, описывающие файл: тип, права, размер, владелец, время модификации и т.д. Примеры:

* `st_mode` (тип + права);
* `st_uid`, `st_gid` (владелец и группа);
* `st_size` (размер);
* `st_atime`, `st_mtime`, `st_ctime` (времена доступа, модификации, изменения метаданных);
* количество жёстких ссылок.

**17. Что означают биты прав доступа (`rwxrwxrwx`)?**

* первые три — права владельца (user),
* следующие три — группы (group),
* последние три — остальных (others).

Каждая тройка:

* `r` — право чтения,
* `w` — право записи,
* `x` — право исполнения (или “входа” в каталог).

**18. Объясните разницу между `mtime`, `atime` и `ctime`.**

* `atime` (access time) — время последнего доступа к файлу (чтения).
* `mtime` (modification time) — время последнего изменения содержимого файла.
* `ctime` (change time) — время изменения метаданных (прав, владельца, размера и т.д.).

**19. Что такое hard link и как он отличается от symbolic link?**

* **Hard link** — дополнительная запись в каталоге, указывающая на тот же `inode`. Все hard link’и равноправны, удаление одного не удаляет данные, пока есть другие ссылки.
* **Symbolic link (symlink)** — отдельный файл специального типа, который содержит путь до целевого файла. Если целевой файл удалён, symlink становится “битым”.

**20. Как FS определяет размер файла?**
Размер хранится в поле `st_size` (в `inode`/`struct stat`). При записи данных FS обновляет это поле. Для регулярного файла это число байт в содержимом. Внутренне FS может хранить размер и в своих метаструктурах (например, в inode на диске).

---

### Физическая организация

**21. Что такое блок в файловой системе?**
Блок — минимальная единица выделения и ввода-вывода в FS. Файл хранится как набор блоков; размер блока обычно 4 KB, 8 KB и т.п. Все операции чтения/записи в конечном итоге оперируют целыми блоками.

**22. Объясните принцип непрерывного размещения файлов (contiguous allocation).**
При непрерывном размещении файл занимает подряд идущие блоки на диске. Это даёт очень высокую скорость последовательного чтения, но плохо подходит для динамического роста и приводит к фрагментации и сложному управлению свободным пространством.

**23. Как работает организация через связанный список блоков (linked allocation)?**
Файл представлен в виде цепочки блоков, каждый блок содержит указатель на следующий. Так легко добавлять новые блоки, но:

* случайный доступ медленный (нужно проходить цепочку);
* меньше надёжность (потеря одного блока может “оборвать” всю цепь).

**24. Что такое индексный узел (inode-based allocation)?**
Это схема, при которой для файла хранится специальная структура (inode или индексный блок), содержащая массив указателей на блоки данных (прямые, косвенные и т.д.). Это позволяет:

* быстро делать случайный доступ,
* уменьшать фрагментацию,
* масштабироваться до больших файлов.

Так работает ext2/ext3/ext4 (с вариациями).

**25. Сравните ext4 и btrfs по структуре хранения данных.**

* **ext4**:

  * классическая inode-ориентированная FS;
  * использует extents для более эффективного хранения больших последовательных блоков;
  * журналирование метаданных;
  * статическая структура (размер тома и inode задаются при создании).

* **btrfs**:

  * копирование-при-записи (copy-on-write);
  * широкое использование B-деревьев;
  * встроенные snapshots, subvolumes, checksumming данных и метаданных;
  * более “богатая” метаструктура, из-за чего сложнее, но гибче ext4.

---

### Производительность и оптимизация

**26. Почему большие файлы читаются быстрее, чем много мелких?**

* Меньше накладных расходов на метаданные (getattr, open, close).
* Меньше seek’ов и переключений контекста.
* Эффективнее работает read-ahead и кеширование.
* В нашем эксперименте FUSE тоже показывает более стабильный throughput на больших файлах, чем при множестве мелких операций.

**27. Что такое read-ahead и как он улучшает производительность?**
Read-ahead — механизм, при котором FS/ядро предвычитывает данные вперёд, предполагая последовательный доступ. Когда приложение запрашивает следующий блок, он уже находится в кеш-памяти, и чтение происходит значительно быстрее, чем с диска.

**28. Как page cache ускоряет доступ к файлам?**
Page cache — область памяти, где ядро кеширует содержимое файлов. При чтении:

* если данные уже в page cache, чтение идёт из RAM;
* при записи данные сначала записываются в кеш и помечаются как “dirty”, а на диск сбрасываются асинхронно.

Это снижает количество реальных обращений к диску и существенно ускоряет операции.

**29. Почему запись на SSD быстрее, чем на HDD?**

* SSD не имеет механической головки и движущихся частей → отсутствуют задержки seek и вращения.
* Время доступа к любому блоку примерно одинаковое и очень маленькое.
* Высокий параллелизм и внутренние контроллеры оптимизируют работу с флеш-ячейками.

**30. Какие параметры влияют на производительность FUSE FS?**

* Частота и тип операций (много мелких файлов vs большие файлы).
* Характер доступа (случайный или последовательный).
* Объём логики и преобразований в пользовательском коде (например, шифрование).
* Наличие и объём кеширования (FUSE может использовать собственный кеш).
* Overhead переключения контекста между ядром и userspace.
* Скорость и тип реальной FS, поверх которой работает FUSE (в нашем случае `tmpfs`).

---

## 6. Выводы

В ходе лабораторной работы были реализованы три варианта FUSE-файловой системы:

1. **Passthrough FS**, прозрачно проксирующая операции в реальную директорию.
2. **ROT13 FS**, шифрующая содержимое файлов на диске и автоматически расшифровывающая его при чтении.
3. **Uppercase FS**, изменяющая только представление данных при чтении.

Были изучены:

* архитектура VFS и FUSE;
* базовые file operations (`getattr`, `readdir`, `open`, `read`, `write`, `create`, `unlink`, `mkdir`, `rmdir`);
* вопросы безопасности (path traversal, права доступа).

Нагрузочное тестирование показало:

* заметный overhead FUSE по сравнению с прямым доступом к tmpfs, особенно на множестве мелких операций (IOPS);
* приемлемую производительность на больших последовательных чтениях/записях;
* характерный для FUSE trade-off: удобство разработки и безопасности в userspace ценой части производительности.

В целом цель работы — понять устройство VFS/FUSE и реализовать пользовательскую файловую систему — достигнута.

---

## 7. Использование AI

AI помог с ответами на вопросы и оформлении отчета.

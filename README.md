# Лабораторная по операционным системам  
**Вариант 2**

**Студент:** Хвойницкий Артём  
**Каталог:** `lab4/gr9sub1/Хвойницкий_Артем/`  
**Сборка:** `make` (`gcc -Wall -Wextra -O2`)

---

## Цели и среда

**Цели**
1. Исследовать виртуальную память процесса и метрики VSZ/RSS/PSS/USS.  
2. Сравнить буферизованный и небуферизованный файловый I/O при разных размерах буфера.  
3. Проанализировать дисковую активность и поведение планировщика I/O под нагрузкой.  
4. (Практика) Реализовать `memory_profiler` (минимум: метрики памяти по PID, карта памяти, динамика).

**Окружение**
- ОС/ядро: Linux `6.14.0-29-generic` (x86_64), VirtualBox; 4 CPU.  
- Компилятор: `gcc` (`-Wall -Wextra -O2`).  
- Инструменты: `/proc`, `ps`, `htop`, `iostat`, `pidstat`, `strace`.

**Структура проекта**
```
lab4/
  gr9sub1/
   Хвойницкий_Артем/
      REPORT.MD
      src/
        memory_profiler.c
        a_memmap.c
        b_io_bench.c
        c_disk_stress.c
      Makefile
      screenshots/
      logs/
```


---

## Задание A — Анализ виртуальной памяти процесса

**Методика.** `a_memmap`:
- выделяет стек, `malloc(1MB)` (heap) и `mmap(1MB, MAP_PRIVATE|MAP_ANONYMOUS)`;  
- «трогает» страницы шагом 4 KB;  
- печатает `/proc/self/status`, `/proc/self/smaps_rollup`, первые 25 строк `/proc/self/maps`;  
- ждёт на паузе для внешних замеров.

**Результаты (из `logs/a_memmap.txt`)**

| Фаза                                  | VSZ (kB) | RSS (kB) | PSS (kB) | USS (kB) |
|---------------------------------------|---------:|---------:|---------:|---------:|
| До выделений                          |   2 748  |   1 580  |   127    |   112    |
| После `malloc(1MB)+mmap(1MB)` + touch |   4 800  |   3 628  |  2 178   |  2 164   |

**Подтверждения**
- `logs/ps_mem.txt`: `VSZ=4800 kB`, `RSS=3628 kB`  
- `logs/status_vm.txt`: `VmSize=4800 kB`, `VmRSS=3628 kB`  
- `logs/smaps_rollup.txt`: `Pss=2178 kB`, `Private_Clean=12 kB`, `Private_Dirty=2152 kB` → **USS=2164 kB**

**Карта памяти (`logs/maps_head.txt`)**
- ELF-сегменты `a_memmap` (r--/r-x/r--/rw).  
- `[heap]` — куча (там `malloc(1MB)`).  
- `libc.so.6`, `ld-linux-x86-64.so.2` — разделяемые библиотеки.  
- `[vdso]`, `[vvar]` — спец-маппинги ядра.  
- Анонимные `rw-p 00:00 0` — среди них `mmap(1MB)`.

**Скриншот**
- `screenshots/htop_a.png`: `VIRT≈4800 kB`, `RES≈3628 kB`.

**Выводы по A**
- VSZ значительно больше RSS (включает всё виртуальное пространство, в т.ч. ленивые/разделяемые маппинги).  
- После «прогрева» страниц растут RSS/PSS/USS; до касания рост минимален.  
- `malloc` и анонимный `mmap` после записи дают сопоставимый прирост RSS.

---

## Задание B — Буферизованный vs небуферизованный I/O

**Методика.** `b_io_bench` пишет 100 MB: `stdio fputc`, а также `write()` с буферами 512 B, 4 KB, 64 KB; время — `clock_gettime`. Дополнительно `strace -c` для подсчёта системных вызовов.

**Результаты (`logs/b_io_bench.txt`)**

| Режим   | Буфер | Время (с) | Скорость (MB/s) | ~write() |
|---------|------:|----------:|----------------:|---------:|
| fputc   |   1 B |   0.442   |       226.06    | n/a      |
| write() | 512 B |   0.543   |       184.15    | ~204 800 |
| write() | 4 KB  |   0.233   |       428.80    | ~25 600  |
| write() | 64 KB |   0.207   |       483.22    | ~1 600   |

**`strace -c` (4 KB, `logs/strace_syswrite_4k.txt`)**
- ≈25 601 `write()`, 1 `fsync`. Почти всё «ядровое» время — в `write`.  
- `strace` замедляет бенч → годится для **подсчёта вызовов**, не для throughput.

**Выводы по B**
- Крупный буфер резко уменьшает число системных вызовов → выше скорость.  
- Основной скачок — 512 B → 4 KB; 64 KB даёт умеренный прирост.  
- Для больших последовательных записей практичны буферы десятки килобайт.

---

## Задание C — Дисковый I/O и планирование

**Методика.** `c_disk_stress` на 5 GB в отдельных прогонах **write/read**; параллельно `iostat -x`, `pidstat -d -p <PID>`, потоковый `/proc/<PID>/io`. Скрины `htop` с I/O-колонками.

**Прямые замеры**
- `logs/c_disk_write.txt`: **WRITE** 5 120.00 MB за **6.587 s** → **777.25 MB/s**  
- `logs/c_disk_read.txt`:  **READ**  5 120.00 MB за **5.611 s** → **912.55 MB/s**

**Мониторинги и скриншоты**
- `screenshots/htop_c_write.png` — DISK WRITE ~600–700 MB/s (сост. `D`).  
- `screenshots/htop_c_read.png` — DISK READ ~750–900 MB/s (сост. `R`).  
- `logs/iostat_read.txt`: пиковые `rkB/s` ~964 115 kB/s (~941 MB/s), `await` ~1–2 ms, `%util` ~55–70%.  
- `logs/pidstat_read.txt`: пик `kB_rd/s` ~1 139 564 (~1.09 GB/s).  
- `logs/proc_io_read_stream.txt`: рост `read_bytes` до ~4.52 GB; `syscr` до ~4.5K.

**Выводы по C**
- Чтение быстрее записи (≈0.91 vs 0.78 GB/s) благодаря readahead и отсутствию обязательного flush.  
- Диск загружен существенно, но не «в потолок» — влияет гипервизор/кэши/CPU.

---

## Вопросы для отчёта (1–30)

1. **Что такое виртуальная память и зачем она нужна?**  
   Абстракция непрерывного адресного пространства процесса: изоляция и защита, удобное распределение, ленивые загрузка/выделение, подкачка, CoW, разделяемые маппинги.

2. **Чем отличаются VSZ, RSS, PSS, USS? Какая метрика точнее для потребления процесса?**  
   VSZ — весь виртуальный объём; RSS — резидентные физические страницы; PSS — RSS с делением общих страниц; USS — только приватные. **USS** — наиболее корректно отражает уникальную память процесса, **PSS** — «справедливо» делит общие.

3. **Что такое страница, кадр, таблица страниц?**  
   Страница — блок виртуальной памяти (обычно 4 KB). Кадр — блок физической памяти. Таблица страниц сопоставляет виртуальные страницы физическим кадрам и правам.

4. **Как работает MMU и TLB? Что при промахе TLB?**  
   MMU транслирует VA→PA; TLB — кэш трансляций. При промахе TLB выполняется обход таблиц (аппаратно/с участием ядра), запись попадает в TLB; доступ дороже.

5. **Что такое Copy-on-Write и где применяется?**  
   Совместное использование страниц только для чтения; первая запись создаёт приватную копию. Применение: `fork()`, `mmap(MAP_PRIVATE)`, загрузка ELF.

6. **Minor vs Major page fault?**  
   Minor — страница уже в памяти/кэше (или zero-page), дисковое I/O не нужно. Major — требуется чтение с диска.

7. **Почему при первом обращении к `malloc()`-памяти происходит page fault?**  
   Ленивая выдача: до доступа выделено только виртуальное пространство; первая запись вызывает минорный fault и реальное выделение кадра.

8. **Как уменьшить количество page faults?**  
   Прогрев (последовательный доступ), `madvise(MADV_WILLNEED)`, `mlock`, большие страницы (THP/HugeTLB), больше RAM, кэш-френдли алгоритмы.

9. **Demand paging и page replacement?**  
   Demand — загрузка страниц при первом обращении. Replacement — выбор страницы на вытеснение при нехватке RAM (LRU-подобные, CLOCK и др.).

10. **`mmap()` vs `read()/write()`; когда `mmap` эффективнее?**  
    `mmap` отображает файл/анонимную память в адресное пространство и использует page cache; эффективен при случайном доступе, разделении между процессами, экономии копирований. `read/write` — явная копия в пользовательский буфер.

11. **`MAP_PRIVATE` vs `MAP_SHARED`?**  
    `MAP_PRIVATE` — CoW, изменения не видны другим; `MAP_SHARED` — изменения видны всем шарящим и могут попадать на диск.

12. **Что будет при обращении за пределы отображённого файла?**  
    Вне маппинга — `SIGSEGV`; в «дыру»/за конец file-backed — часто `SIGBUS`.

13. **Как работает page cache и зачем он нужен?**  
    Кэш страниц файлов/блоков в RAM: ускоряет повторы чтения, агрегирует записи, сглаживает I/O, позволяет отложенный сброс.

14. **Зачем буферизация и её уровни?**  
    Снижает количество системных/дисковых операций, выравнивает блоки. Уровни: user-space (stdio), kernel (page cache), устройство/контроллер.

15. **Почему маленький буфер тормозит I/O?**  
    Много `write()/read()` → высокие накладные расходы на syscalls/переключения, плохая амортизация.

16. **`O_DIRECT` и `O_SYNC` — что это и когда?**  
    `O_DIRECT` минует page cache (жёсткие выравнивания) — для БД/собственных кэшей. `O_SYNC` завершает запись после фактического размещения на носителе — для строгой долговечности.

17. **`fwrite()` vs `write()`?**  
    `fwrite` — буферизованная stdio-функция в user-space; `write` — системный вызов в ядро.

18. **Что такое inode и что в нём хранится?**  
    Метаданные файла: права, владелец, размеры, метки времени, указатели на блоки/extent-ы. Имени файла в inode нет.

19. **Почему имя файла не в inode?**  
    Имена хранятся в каталоге (записи «имя → inode»), что позволяет нескольким именам ссылаться на один inode (hard links).

20. **Жёсткая vs символическая ссылка?**  
    Hard link — ещё одно имя того же inode; файл жив, пока есть хотя бы одна ссылка. Symlink — отдельный файл-указатель на путь (может «висеть» в никуда).

21. **Как в ext4 хранятся большие файлы (indirect pointers)?**  
    Исторически — прямые/косвенные/двойные/тройные указатели; в ext4 повсеместно используются **extents** (диапазоны блоков), эффективнее для больших файлов.

22. **Зачем нужны I/O-планировщики?**  
    Упорядочивают запросы для повышения пропускной способности, снижения латентности и обеспечения справедливости.

23. **FCFS, SSTF, SCAN — различия?**  
    FCFS — по порядку прихода; SSTF — ближайший запрос (риск голодания дальних); SCAN — «лифт» туда-обратно (баланс задержек).

24. **Какие I/O-планировщики в современном Linux?**  
    `mq-deadline`, `bfq`, `kyber`; для NVMe часто `none`/`noop`.

25. **Почему для SSD планирование менее критично, чем для HDD?**  
    Нет seek/rotation; важнее очередь и параллельность контроллера, чем порядок цилиндров.

26. **Фрагментация памяти: внутренняя vs внешняя.**  
    Внутренняя — незадействованные байты внутри блока (округления/слэбы). Внешняя — память разбита на мелкие участки, сложно выделить крупный непрерывный.

27. **Как swap влияет на производительность?**  
    Помогает вместить рабочие наборы, но активная подкачка повышает задержки; постоянные page-in/out приводят к thrashing.

28. **Что такое thrashing и как избежать?**  
    CPU занят подкачкой вместо работы (рабочий набор > RAM). Решения: больше RAM, ограничить параллелизм, улучшить локальность, тюнинг свопа/oom-политик.

29. **Почему последовательный доступ быстрее случайного?**  
    Лучшая локальность: эффективнее кэш/предвыборка, меньше TLB-промахов; на диске — минимум позиционирования.

30. **Что такое cache-friendly код?**  
    Код с высокой локальностью: последовательные проходы, блочное разбиение, упаковка структур, работа порциями, учитывающими размеры кэша/страниц/кэшей CPU.

---

## Итоговые выводы
1. После касания страниц (touch) RSS/PSS/USS растут ожидаемо; VSZ выше всегда, т.к. это всё виртуальное пространство.  
2. Для I/O крупные буферы (4–64 KB) резко снижают накладные расходы и повышают скорость.  
3. На стенде последовательное чтение (~0.9–1.1 GB/s) быстрее записи (~0.6–0.8 GB/s); мониторинги согласованы.  
4. `/proc`, `htop`, `iostat`, `pidstat` дают целостную картину; `strace` — для счёта вызовов, не для измерения throughput.

---

## Приложения (артефакты)
- `logs/a_memmap.txt`, `logs/ps_mem.txt`, `logs/status_vm.txt`, `logs/smaps_rollup.txt`, `logs/maps_head.txt`  
- `logs/b_io_bench.txt`, `logs/strace_syswrite_4k.txt`  
- `logs/c_disk_write.txt`, `logs/c_disk_read.txt`, `logs/iostat_read.txt`, `logs/pidstat_read.txt`, `logs/proc_io_read_stream.txt`  
- `screenshots/htop_a.png`, `screenshots/htop_c_write.png`, `screenshots/htop_c_read.png`

---

## Использование ИИ
ИИ использовался для помощи в оформлении отчета и ответов на вопросы.
